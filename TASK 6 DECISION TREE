# IMPORTING REQUIRED LIBRARIES

import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

from sklearn import tree
%matplotlib inline

from sklearn import metrics



# READING DATA

data=pd.read_csv("Iris.csv")

data.head()

data.info()



# COMMON DATASET EXPLORATION CHECKING FOR SHAPE,NULL,MISSING VALUES

#Checking shape of Dataset
data.shape

#Checking for any missing value
data.isnull().sum()

#Looking for short description of dataset
data.describe()



# SPLITTING DATA INTO TRAINING AND TESTING

features = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']

#Creat features matrix
x=data.loc[:,features].values
y=data.Species
x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=0)



# DECISION TREE ALGORITHM

clf = DecisionTreeClassifier(max_depth=2, random_state=0)
clf.fit(x_train, y_train)

# Pedict for Multiple Observations at Once
clf.predict(x_test[0:1])

# Measuring the Model Performance
score=clf.score(x_test,y_test)
print(score)
print(metrics.classification_report(y_test,clf.predict(x_test)))

cm=metrics.confusion_matrix(y_test,clf.predict(x_test))

plt.figure(figsize=(7,7))
sns.heatmap(cm,annot=True,
           fmt=".0f",
           linewidth=.5,
           square=True,
           cmap='Blues');
plt.ylabel('Actual label',fontsize=17);
plt.xlabel('Predicted label',fontsize=17);
plt.tick_params(labelsize=15)



# Finding optimal max_depth

# List of values to try for max_depth
max_depth_range=list(range(1,6))

#List to store the average RMSE for each value of max_depth
accuracy=[]

for depth in max_depth_range:
    clf=DecisionTreeClassifier(max_depth=depth,
                              random_state=0)
    clf.fit(x_train,y_train)
    score=clf.score(x_test,y_test)
    accuracy.append(score)
    
    # Plotting accuracy score depth wise
fig, ax=plt.subplots(nrows=1,ncols=1,figsize=(10,5));
ax.plot(max_depth_range,
       accuracy,
       lw=2,
       color='k')
ax.set_xlim([1,5])
ax.set_ylim([.50,1.00])
ax.grid(True,axis='both',
        zorder=0,
       linestyle=':',
       color='k')

ax.tick_params(labelsize=10)
ax.set_xticks([1,2,3,4,5])
ax.set_xlabel('max_depth',fontsize=24)
ax.set_ylabel('Accuracy',fontsize=24)
fig.tight_layout()



#VISUALIZING DECISION TREES

fig, axes=plt.subplots(nrows=1,ncols=1,figsize=(7,4),dpi=150)
tree.plot_tree(clf);

# MAKE TREE MORE INTERPRETABLE

# Putting the feature names and class names into variables
fn=['sepal length (cm)','Sepal width (cm)','Petal length (cm)','Petal width (cm)']
cn=['setosa','versicolor','verginica']

fig, axes=plt.subplots(nrows=1,ncols=1,figsize=(7,4),dpi=300)
tree.plot_tree(clf,
              feature_names=fn,
              class_names=cn,
              filled=True);
              
